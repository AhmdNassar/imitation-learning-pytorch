{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A.Nassar\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms \n",
    "from torch import optim\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "from model import CarlaNet\n",
    "from utils import visualizeImages , cost ,loss_mask  , StopEarly  ,load_saved_model\n",
    "from data import AgentData , ToTensor \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "BRANCH_LOSS_WEIGHT= [0.95, 0.95, 0.95, 0.95] # how much each branch is weighted when computing loss\n",
    "# params used in loss, we will update this None values while training \n",
    "params = {\"branch_weights\":BRANCH_LOSS_WEIGHT,\\\n",
    "          # how much each of the outputs specified on TARGETS are weighted for learning.\n",
    "         \"variable_weights\":{\"Steer\":0.5,\"Gas\":0.45,\"Brake\":0.05},\\\n",
    "          # values we used them while training \n",
    "         \"branches\":None , \"targets\": None , \"controls_mask\":None}\n",
    "loss_type = \"L1\"\n",
    "model = CarlaNet().cuda()\n",
    "opt = optim.SGD(model.parameters(),lr=0.01,momentum=0.9,nesterov=True)\n",
    "stop_early = StopEarly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ToTensor()])\n",
    "training_data = AgentData('./SeqTrain',transforms=transform)\n",
    "train_loader = DataLoader(training_data,batch_size=batch_size)\n",
    "data_iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of trainin samples is : 226000\n"
     ]
    }
   ],
   "source": [
    "# info about Data \n",
    "print(f\"num of trainin samples is : {len(training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_batches = len(training_data)/batch_size\n",
    "num_of_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(n_epochs):\n",
    "    \n",
    "    #check if we have saved checkpoint and load it if exist\n",
    "    load_saved_model(model)\n",
    "    # prepare the net for training\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # train on batches of data, assumes you already have train_loader\n",
    "        for batch_i, data in enumerate(train_loader):\n",
    "            # get inputs(img,speed,command) and targets(steer,gas,brake)\n",
    "            inputs ,targets = data\n",
    "            imgs , speed , command = inputs\n",
    "            imgs , speed , command = imgs.type(torch.cuda.FloatTensor) ,speed.type(torch.cuda.FloatTensor)\\\n",
    "                                    ,command.type(torch.cuda.LongTensor)\n",
    "            \n",
    "            speed , command = speed.view((batch_size,1)) , command.view((batch_size,1))\n",
    "            inputs = (imgs , speed , command)\n",
    "            \n",
    "            # targets \n",
    "            steer , gas , brake = targets\n",
    "            targets = torch.stack([steer,gas,brake],dim=1).cuda()\n",
    "\n",
    "            # forward pass to get outputs\n",
    "            outs = model(inputs)\n",
    "            controls_masks = loss_mask(command)\n",
    "            \n",
    "            #update params\n",
    "            params[\"branches\"] = outs\n",
    "            params[\"targets\"] = targets\n",
    "            params[\"controls_mask\"] = controls_masks\n",
    "\n",
    "            # calculate the loss between predicted and target keypoints\n",
    "            loss = cost(params=params,type_loss=loss_type)\n",
    "\n",
    "            # zero the parameter (weight) gradients\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # backward pass to calculate the weight gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update the weights\n",
    "            opt.step()\n",
    "\n",
    "            # print loss statistics\n",
    "            # to convert loss into a scalar and add it to the running_loss, use .item()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += running_loss\n",
    "            if (batch_i +1) % 100 == 0:    # print every 100 batches\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, running_loss/100))\n",
    "                running_loss = 0.0\n",
    "        print('----------------\\nEpoch loss: {}\\n------------\\n'.format(epoch_loss/num_of_batches))\n",
    "        if stop_early(model,epoch_loss/num_of_batches) :\n",
    "            print(\"Early stop activated ... stoping training process..\")\n",
    "            break\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded.\n",
      "\n",
      "Epoch: 1, Batch: 100, Avg. Loss: 0.0018992124107899144\n",
      "Epoch: 1, Batch: 200, Avg. Loss: 0.0018389033508719877\n",
      "Epoch: 1, Batch: 300, Avg. Loss: 0.0018614178083953448\n",
      "Epoch: 1, Batch: 400, Avg. Loss: 0.0022299541140091608\n",
      "Epoch: 1, Batch: 500, Avg. Loss: 0.0032677691581193356\n",
      "Epoch: 1, Batch: 600, Avg. Loss: 0.003032795008912217\n",
      "Epoch: 1, Batch: 700, Avg. Loss: 0.00314891014539171\n",
      "Epoch: 1, Batch: 800, Avg. Loss: 0.0032294048025505616\n",
      "Epoch: 1, Batch: 900, Avg. Loss: 0.003066960951546207\n",
      "Epoch: 1, Batch: 1000, Avg. Loss: 0.0034852452372433618\n",
      "Epoch: 1, Batch: 1100, Avg. Loss: 0.002393294073990546\n",
      "----------------\n",
      "Epoch loss: 0.13279929974016452\n",
      "------------\n",
      "\n",
      "Epoch: 2, Batch: 100, Avg. Loss: 0.0016854530840646476\n",
      "Epoch: 2, Batch: 200, Avg. Loss: 0.0018373145427904092\n",
      "Epoch: 2, Batch: 300, Avg. Loss: 0.001780009117210284\n",
      "Epoch: 2, Batch: 400, Avg. Loss: 0.0022529468542779795\n",
      "Epoch: 2, Batch: 500, Avg. Loss: 0.003226497326104436\n",
      "Epoch: 2, Batch: 600, Avg. Loss: 0.0029484200367005543\n",
      "Epoch: 2, Batch: 700, Avg. Loss: 0.0030796075123362245\n",
      "Epoch: 2, Batch: 800, Avg. Loss: 0.0030787615827284755\n",
      "Epoch: 2, Batch: 900, Avg. Loss: 0.0030632862966740505\n",
      "Epoch: 2, Batch: 1000, Avg. Loss: 0.003501365158590488\n",
      "Epoch: 2, Batch: 1100, Avg. Loss: 0.002354154688073322\n",
      "----------------\n",
      "Epoch loss: 0.12993378998630317\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.12993378998630317 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Epoch: 3, Batch: 100, Avg. Loss: 0.0016200102784205228\n",
      "Epoch: 3, Batch: 200, Avg. Loss: 0.0015639120672130958\n",
      "Epoch: 3, Batch: 300, Avg. Loss: 0.0015578000602545217\n",
      "Epoch: 3, Batch: 400, Avg. Loss: 0.0019842159005929714\n",
      "Epoch: 3, Batch: 500, Avg. Loss: 0.0031247808976331724\n",
      "Epoch: 3, Batch: 600, Avg. Loss: 0.0028485504316631705\n",
      "Epoch: 3, Batch: 700, Avg. Loss: 0.0030260964285116643\n",
      "Epoch: 3, Batch: 800, Avg. Loss: 0.0029595208502723835\n",
      "Epoch: 3, Batch: 900, Avg. Loss: 0.0029893985713715667\n",
      "Epoch: 3, Batch: 1000, Avg. Loss: 0.003552159861137625\n",
      "Epoch: 3, Batch: 1100, Avg. Loss: 0.0022978524421341717\n",
      "----------------\n",
      "Epoch loss: 0.1243134790075947\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.1243134790075947 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Epoch: 4, Batch: 100, Avg. Loss: 0.0015102926446706989\n",
      "Epoch: 4, Batch: 200, Avg. Loss: 0.0014489542771480045\n",
      "Epoch: 4, Batch: 300, Avg. Loss: 0.0014692941543762571\n",
      "Epoch: 4, Batch: 400, Avg. Loss: 0.0018539592460729182\n",
      "Epoch: 4, Batch: 500, Avg. Loss: 0.003090313296415843\n",
      "Epoch: 4, Batch: 600, Avg. Loss: 0.0028334664754220283\n",
      "Epoch: 4, Batch: 700, Avg. Loss: 0.002960631819150876\n",
      "Epoch: 4, Batch: 800, Avg. Loss: 0.0029824956487573217\n",
      "Epoch: 4, Batch: 900, Avg. Loss: 0.003023594957194291\n",
      "Epoch: 4, Batch: 1000, Avg. Loss: 0.0034245367083349265\n",
      "Epoch: 4, Batch: 1100, Avg. Loss: 0.0021501784707652406\n",
      "----------------\n",
      "Epoch loss: 0.12077813948744412\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.12077813948744412 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Epoch: 5, Batch: 100, Avg. Loss: 0.0014864040486281737\n",
      "Epoch: 5, Batch: 200, Avg. Loss: 0.00144729464314878\n",
      "Epoch: 5, Batch: 300, Avg. Loss: 0.0015132088420796207\n",
      "Epoch: 5, Batch: 400, Avg. Loss: 0.0018502531651756727\n",
      "Epoch: 5, Batch: 500, Avg. Loss: 0.003096494130440988\n",
      "Epoch: 5, Batch: 600, Avg. Loss: 0.002729149721271824\n",
      "Epoch: 5, Batch: 700, Avg. Loss: 0.002979301270679571\n",
      "Epoch: 5, Batch: 800, Avg. Loss: 0.0029611523778294213\n",
      "Epoch: 5, Batch: 900, Avg. Loss: 0.00297358352370793\n",
      "Epoch: 5, Batch: 1000, Avg. Loss: 0.00339962773519801\n",
      "Epoch: 5, Batch: 1100, Avg. Loss: 0.0021565377482329496\n",
      "----------------\n",
      "Epoch loss: 0.12003497066982276\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.12003497066982276 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Epoch: 6, Batch: 100, Avg. Loss: 0.0014581729540077504\n",
      "Epoch: 6, Batch: 200, Avg. Loss: 0.0013457037060288712\n",
      "Epoch: 6, Batch: 300, Avg. Loss: 0.0013682540775334928\n",
      "Epoch: 6, Batch: 400, Avg. Loss: 0.0018295702437171712\n",
      "Epoch: 6, Batch: 500, Avg. Loss: 0.0030098793032811953\n",
      "Epoch: 6, Batch: 600, Avg. Loss: 0.0026967326321755535\n",
      "Epoch: 6, Batch: 700, Avg. Loss: 0.002851265572826378\n",
      "Epoch: 6, Batch: 800, Avg. Loss: 0.0029083283623913304\n",
      "Epoch: 6, Batch: 900, Avg. Loss: 0.002951425939827459\n",
      "Epoch: 6, Batch: 1000, Avg. Loss: 0.00337198337627342\n",
      "Epoch: 6, Batch: 1100, Avg. Loss: 0.0021190245848265476\n",
      "----------------\n",
      "Epoch loss: 0.11695221483610072\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.11695221483610072 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Epoch: 7, Batch: 100, Avg. Loss: 0.0013088668060663622\n",
      "Epoch: 7, Batch: 200, Avg. Loss: 0.0012880396642140112\n",
      "Epoch: 7, Batch: 300, Avg. Loss: 0.0013277907017618417\n",
      "Epoch: 7, Batch: 400, Avg. Loss: 0.001740016004332574\n",
      "Epoch: 7, Batch: 500, Avg. Loss: 0.0030524479543964844\n",
      "Epoch: 7, Batch: 600, Avg. Loss: 0.002697819752502255\n",
      "Epoch: 7, Batch: 700, Avg. Loss: 0.0028393092929036357\n",
      "Epoch: 7, Batch: 800, Avg. Loss: 0.002904226205428131\n",
      "Epoch: 7, Batch: 900, Avg. Loss: 0.002920511119591538\n",
      "Epoch: 7, Batch: 1000, Avg. Loss: 0.0034202976353117265\n",
      "Epoch: 7, Batch: 1100, Avg. Loss: 0.0021246781630907207\n",
      "----------------\n",
      "Epoch loss: 0.11566119891930314\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.11566119891930314 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Epoch: 8, Batch: 100, Avg. Loss: 0.0013188417605124415\n",
      "Epoch: 8, Batch: 200, Avg. Loss: 0.001210044205217855\n",
      "Epoch: 8, Batch: 300, Avg. Loss: 0.0012007693079067394\n",
      "Epoch: 8, Batch: 400, Avg. Loss: 0.0017520360526395962\n",
      "Epoch: 8, Batch: 500, Avg. Loss: 0.0030301240022527053\n",
      "Epoch: 8, Batch: 600, Avg. Loss: 0.0026672010944457725\n",
      "Epoch: 8, Batch: 700, Avg. Loss: 0.0027922374752233736\n",
      "Epoch: 8, Batch: 800, Avg. Loss: 0.0028887881401169577\n",
      "Epoch: 8, Batch: 900, Avg. Loss: 0.002943403806129936\n",
      "Epoch: 8, Batch: 1000, Avg. Loss: 0.003352423440956045\n",
      "Epoch: 8, Batch: 1100, Avg. Loss: 0.002098432717612013\n",
      "----------------\n",
      "Epoch loss: 0.11410664206876037\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.11410664206876037 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Epoch: 9, Batch: 100, Avg. Loss: 0.0012314282523584552\n",
      "Epoch: 9, Batch: 200, Avg. Loss: 0.0011798257686314172\n",
      "Epoch: 9, Batch: 300, Avg. Loss: 0.0011959283193573355\n",
      "Epoch: 9, Batch: 400, Avg. Loss: 0.0016910856918548234\n",
      "Epoch: 9, Batch: 500, Avg. Loss: 0.002978130149131175\n",
      "Epoch: 9, Batch: 600, Avg. Loss: 0.002697934880561661\n",
      "Epoch: 9, Batch: 700, Avg. Loss: 0.002816562861589773\n",
      "Epoch: 9, Batch: 800, Avg. Loss: 0.002895265738334274\n",
      "Epoch: 9, Batch: 900, Avg. Loss: 0.002867569410300348\n",
      "Epoch: 9, Batch: 1000, Avg. Loss: 0.003328863201895729\n",
      "Epoch: 9, Batch: 1100, Avg. Loss: 0.002079809102870058\n",
      "----------------\n",
      "Epoch loss: 0.11289508825897734\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.11289508825897734 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Epoch: 10, Batch: 100, Avg. Loss: 0.0012091718146984931\n",
      "Epoch: 10, Batch: 200, Avg. Loss: 0.0011517391905363184\n",
      "Epoch: 10, Batch: 300, Avg. Loss: 0.0012424614072369877\n",
      "Epoch: 10, Batch: 400, Avg. Loss: 0.0016472317931766156\n",
      "Epoch: 10, Batch: 500, Avg. Loss: 0.0029612855723826215\n",
      "Epoch: 10, Batch: 600, Avg. Loss: 0.0026371385712991467\n",
      "Epoch: 10, Batch: 700, Avg. Loss: 0.002773887752846349\n",
      "Epoch: 10, Batch: 800, Avg. Loss: 0.002848465838906122\n",
      "Epoch: 10, Batch: 900, Avg. Loss: 0.0028853431572497357\n",
      "Epoch: 10, Batch: 1000, Avg. Loss: 0.0033786039019469173\n",
      "Epoch: 10, Batch: 1100, Avg. Loss: 0.0020537110486475284\n",
      "----------------\n",
      "Epoch loss: 0.11222120655620207\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.11222120655620207 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Epoch: 11, Batch: 100, Avg. Loss: 0.001186301144771278\n",
      "Epoch: 11, Batch: 200, Avg. Loss: 0.0011236738527077251\n",
      "Epoch: 11, Batch: 300, Avg. Loss: 0.0011406177206663415\n",
      "Epoch: 11, Batch: 400, Avg. Loss: 0.0016269907911191693\n",
      "Epoch: 11, Batch: 500, Avg. Loss: 0.002910312448657351\n",
      "Epoch: 11, Batch: 600, Avg. Loss: 0.0026503853168105707\n",
      "Epoch: 11, Batch: 700, Avg. Loss: 0.002737617936800234\n",
      "Epoch: 11, Batch: 800, Avg. Loss: 0.0028947519010398535\n",
      "Epoch: 11, Batch: 900, Avg. Loss: 0.0028901999202207663\n",
      "Epoch: 11, Batch: 1000, Avg. Loss: 0.003296073626552243\n",
      "Epoch: 11, Batch: 1100, Avg. Loss: 0.0020631946608773433\n",
      "----------------\n",
      "Epoch loss: 0.11115784421473004\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.11115784421473004 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Epoch: 12, Batch: 100, Avg. Loss: 0.001181009720239672\n",
      "Epoch: 12, Batch: 200, Avg. Loss: 0.0011148659596801735\n",
      "Epoch: 12, Batch: 300, Avg. Loss: 0.0011408744700020178\n",
      "Epoch: 12, Batch: 400, Avg. Loss: 0.0016683602961711586\n",
      "Epoch: 12, Batch: 500, Avg. Loss: 0.0029515095530223334\n",
      "Epoch: 12, Batch: 600, Avg. Loss: 0.002617057758179726\n",
      "Epoch: 12, Batch: 700, Avg. Loss: 0.0027296279354777652\n",
      "Epoch: 12, Batch: 800, Avg. Loss: 0.0028701176797039808\n",
      "Epoch: 12, Batch: 900, Avg. Loss: 0.002856684316211613\n",
      "Epoch: 12, Batch: 1000, Avg. Loss: 0.003268560631549917\n",
      "Epoch: 12, Batch: 1100, Avg. Loss: 0.0020183238992467524\n",
      "----------------\n",
      "Epoch loss: 0.11040203683719603\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.11040203683719603 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Epoch: 13, Batch: 100, Avg. Loss: 0.0011495317588560282\n",
      "Epoch: 13, Batch: 200, Avg. Loss: 0.001085895793949021\n",
      "Epoch: 13, Batch: 300, Avg. Loss: 0.0011216549009259325\n",
      "Epoch: 13, Batch: 400, Avg. Loss: 0.001552336518507218\n",
      "Epoch: 13, Batch: 500, Avg. Loss: 0.0029589722339005676\n",
      "Epoch: 13, Batch: 600, Avg. Loss: 0.0025964836549246684\n",
      "Epoch: 13, Batch: 700, Avg. Loss: 0.0027068735788634514\n",
      "Epoch: 13, Batch: 800, Avg. Loss: 0.0028576195432106033\n",
      "Epoch: 13, Batch: 900, Avg. Loss: 0.002846414349623956\n",
      "Epoch: 13, Batch: 1000, Avg. Loss: 0.003270185300061712\n",
      "Epoch: 13, Batch: 1100, Avg. Loss: 0.0020289727013005176\n",
      "----------------\n",
      "Epoch loss: 0.10976398643614896\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.10976398643614896 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Epoch: 14, Batch: 100, Avg. Loss: 0.001124672177102184\n",
      "Epoch: 14, Batch: 200, Avg. Loss: 0.0010733098132186568\n",
      "Epoch: 14, Batch: 300, Avg. Loss: 0.00106970943059423\n",
      "Epoch: 14, Batch: 400, Avg. Loss: 0.0015696146582195069\n",
      "Epoch: 14, Batch: 500, Avg. Loss: 0.002917408607027028\n",
      "Epoch: 14, Batch: 600, Avg. Loss: 0.0026200012087065263\n",
      "Epoch: 14, Batch: 700, Avg. Loss: 0.002727714472421212\n",
      "Epoch: 14, Batch: 800, Avg. Loss: 0.0028432498256734107\n",
      "Epoch: 14, Batch: 900, Avg. Loss: 0.002856370613590116\n",
      "Epoch: 14, Batch: 1000, Avg. Loss: 0.003297610677254852\n",
      "Epoch: 14, Batch: 1100, Avg. Loss: 0.001988679407077143\n",
      "----------------\n",
      "Epoch loss: 0.10908539902606353\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.10908539902606353 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Epoch: 15, Batch: 100, Avg. Loss: 0.0011226879402966006\n",
      "Epoch: 15, Batch: 200, Avg. Loss: 0.0010487228161946405\n",
      "Epoch: 15, Batch: 300, Avg. Loss: 0.0010366037939820672\n",
      "Epoch: 15, Batch: 400, Avg. Loss: 0.0015466548231779598\n",
      "Epoch: 15, Batch: 500, Avg. Loss: 0.0029201634952187305\n",
      "Epoch: 15, Batch: 600, Avg. Loss: 0.0025963049312122165\n",
      "Epoch: 15, Batch: 700, Avg. Loss: 0.002712337907287292\n",
      "Epoch: 15, Batch: 800, Avg. Loss: 0.002836114662641194\n",
      "Epoch: 15, Batch: 900, Avg. Loss: 0.0028665233650826847\n",
      "Epoch: 15, Batch: 1000, Avg. Loss: 0.003293943320895778\n",
      "Epoch: 15, Batch: 1100, Avg. Loss: 0.0019832898503227623\n",
      "----------------\n",
      "Epoch loss: 0.1089729812081859\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.1089729812081859 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Epoch: 16, Batch: 100, Avg. Loss: 0.001100971712730825\n",
      "Epoch: 16, Batch: 200, Avg. Loss: 0.0010411381865560543\n",
      "Epoch: 16, Batch: 300, Avg. Loss: 0.0010321446997113527\n",
      "Epoch: 16, Batch: 400, Avg. Loss: 0.0015454821164166787\n",
      "Epoch: 16, Batch: 500, Avg. Loss: 0.0029158861297764816\n",
      "Epoch: 16, Batch: 600, Avg. Loss: 0.0025851135980337857\n",
      "Epoch: 16, Batch: 700, Avg. Loss: 0.0027077902817109136\n",
      "Epoch: 16, Batch: 800, Avg. Loss: 0.002837759215472033\n",
      "Epoch: 16, Batch: 900, Avg. Loss: 0.002844753951721941\n",
      "Epoch: 16, Batch: 1000, Avg. Loss: 0.003338797344476916\n",
      "Epoch: 16, Batch: 1100, Avg. Loss: 0.0019927286238817033\n",
      "----------------\n",
      "Epoch loss: 0.1086568433230442\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.1086568433230442 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Epoch: 17, Batch: 100, Avg. Loss: 0.0010955957323676557\n",
      "Epoch: 17, Batch: 200, Avg. Loss: 0.0010116302013921086\n",
      "Epoch: 17, Batch: 300, Avg. Loss: 0.00102540846564807\n",
      "Epoch: 17, Batch: 400, Avg. Loss: 0.0015278550726361573\n",
      "Epoch: 17, Batch: 500, Avg. Loss: 0.002893095834297128\n",
      "Epoch: 17, Batch: 600, Avg. Loss: 0.0025744573216070423\n",
      "Epoch: 17, Batch: 700, Avg. Loss: 0.0026706667663529515\n",
      "Epoch: 17, Batch: 800, Avg. Loss: 0.002844631440457306\n",
      "Epoch: 17, Batch: 900, Avg. Loss: 0.0028476418828358874\n",
      "Epoch: 17, Batch: 1000, Avg. Loss: 0.0032674964000761974\n",
      "Epoch: 17, Batch: 1100, Avg. Loss: 0.001972753451045719\n",
      "----------------\n",
      "Epoch loss: 0.10789872761221155\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.10789872761221155 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Epoch: 18, Batch: 100, Avg. Loss: 0.001076258896937361\n",
      "Epoch: 18, Batch: 200, Avg. Loss: 0.0009886966851627222\n",
      "Epoch: 18, Batch: 300, Avg. Loss: 0.0010370921302819624\n",
      "Epoch: 18, Batch: 400, Avg. Loss: 0.0015337091721812613\n",
      "Epoch: 18, Batch: 500, Avg. Loss: 0.002942871112463763\n",
      "Epoch: 18, Batch: 600, Avg. Loss: 0.002553163883567322\n",
      "Epoch: 18, Batch: 700, Avg. Loss: 0.002702837080869358\n",
      "Epoch: 18, Batch: 800, Avg. Loss: 0.0028337811853998574\n",
      "Epoch: 18, Batch: 900, Avg. Loss: 0.0028409611756796947\n",
      "Epoch: 18, Batch: 1000, Avg. Loss: 0.003275151646012091\n",
      "Epoch: 18, Batch: 1100, Avg. Loss: 0.0019675457260746043\n",
      "----------------\n",
      "Epoch loss: 0.10798616147843762\n",
      "------------\n",
      "\n",
      "/nno improvement for 1 / 10.\n",
      "------------\n",
      "Epoch: 19, Batch: 100, Avg. Loss: 0.0010531349696248072\n",
      "Epoch: 19, Batch: 200, Avg. Loss: 0.0009717462566914037\n",
      "Epoch: 19, Batch: 300, Avg. Loss: 0.001003186317466316\n",
      "Epoch: 19, Batch: 400, Avg. Loss: 0.0015168332364555682\n",
      "Epoch: 19, Batch: 500, Avg. Loss: 0.002900037445433554\n",
      "Epoch: 19, Batch: 600, Avg. Loss: 0.002553091684385436\n",
      "Epoch: 19, Batch: 700, Avg. Loss: 0.00268189847956819\n",
      "Epoch: 19, Batch: 800, Avg. Loss: 0.002849346736620646\n",
      "Epoch: 19, Batch: 900, Avg. Loss: 0.0028353138192323967\n",
      "Epoch: 19, Batch: 1000, Avg. Loss: 0.0032877577090403066\n",
      "Epoch: 19, Batch: 1100, Avg. Loss: 0.001953831784630893\n",
      "----------------\n",
      "Epoch loss: 0.10703793169367302\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.10703793169367302 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Epoch: 20, Batch: 100, Avg. Loss: 0.0010432389279594644\n",
      "Epoch: 20, Batch: 200, Avg. Loss: 0.0009437564070685767\n",
      "Epoch: 20, Batch: 300, Avg. Loss: 0.0010105076249601551\n",
      "Epoch: 20, Batch: 400, Avg. Loss: 0.001470141817262629\n",
      "Epoch: 20, Batch: 500, Avg. Loss: 0.0028913833160913783\n",
      "Epoch: 20, Batch: 600, Avg. Loss: 0.0025484351100749335\n",
      "Epoch: 20, Batch: 700, Avg. Loss: 0.0026614207449893\n",
      "Epoch: 20, Batch: 800, Avg. Loss: 0.0028256946519832128\n",
      "Epoch: 20, Batch: 900, Avg. Loss: 0.0028270910933497363\n",
      "Epoch: 20, Batch: 1000, Avg. Loss: 0.0032483983322163113\n",
      "Epoch: 20, Batch: 1100, Avg. Loss: 0.0019558090859936785\n",
      "----------------\n",
      "Epoch loss: 0.10661746293495139\n",
      "------------\n",
      "\n",
      "We get new best loss: 0.10661746293495139 , saving model...\n",
      "model saved.\n",
      "------------\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_net(20) # test train with 20 + 3 in the previse session = 23 iter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
